\section{Sampling}
It is often needed to sample from a joint distribution, of the form $P(y_1, \cdots, y_n)$ or $P(\mathbf x = x_1, \cdots, x_n)$ or $P(x_1, \cdots, x_r | x_{r+1} = E_{r+1}, \cdots, x_n = E_n)$. Some scenarios might be
\begin{enumerate}
	\item Solving an intractable inference during training
	\item Showing a diverse set of outputs instead of just the most likely value
	\item Calculating the expected value of some arbitrary function $f(\mathbf x)$ under distribution $P(\mathbf x)$.
\end{enumerate}
\subsection{Motivation}
\begin{itemize}
	\item[$\diamond$] Say we have a deep language model, we might want to generate sample sentences, questions or expected distribution of first word for sentences ending with '?'.
	\item [$\diamond$] We can use VAEs for missing value imputation. This can be done by fixing values of some of the outputs, and generate most likely values of others.  
\end{itemize}
We had seen in VAEs how to approximate the expected value of a function with sampling. Say we have a function $f(\mathbf x): \mathcal{X} \to \mathbb{R}$ and we want $\mathbb{E}_{P(\mathbf x)}[f(\mathbf x)] = \sum_{\mathbf x \in \mathcal X} f(\mathbf x) P(\mathbf x)$. In case of continuous $\mathbf x$, we can take the integral. Our space $\mathcal X$ is very large, and we cannot compute the integral exactly in closed form. Hence we want to sample and approximate the expectation. Say we have samples $\mathbf x^1, \cdots, \mathbf x^M \sim P(\mathbf x)$, we write
\begin{equation}
	\mathbb{E}_{P(\mathbf x)}[f(\mathbf x)] = \sum_{\mathbf x \in \mathcal X} f(\mathbf x) P(\mathbf x) \approx \dfrac{1}{M} \sum_{i=1}^M f(\mathbf x^i)
\end{equation}
As $M \to \infty$, this approximation matches exact expected value.
\subsection{Sampling scalar distributions}
Let $p(x)$ be a distribution, and we want to draw samples $x^1, \cdots, x^M$. Say we can sample $u$ from $U(0,1)$ (uniform distribution). Let $F(x)$ be the CDF of $p(x)$. \\
\noindent For $i=1, \cdots, M$ \\
$\triangleright$ Sample $u_i \sim U(0,1)$ \\
$\triangleright$ Find $x_i = F^{-1}(u)$ \\
\noindent Now, say we a multinomial distribution. Say $x$ is discrete and $\in \{1, \cdots, m\}$. We have $p(x) \sim Mult(p_1, \cdots, p_m)$, $u_i \sim U(0,1)$, and if $u_i$ is between $\sum_{j=0}^{k-1} p_j$ and $\sum_{j=0}^{k} p_j$, choose $k$. 

Now, as $M \to \infty$, the fraction of times we encounter a sample in the interval $[x, x+\Delta)$ would be proportional to the true probability of that in the interval in $p(x)$, i.e $F(x+\Delta) - F(x)$.
\subsection{Sampling multivariate distributions}
One option to sample from multivariate distributions is to factorize the distribution as a Bayesian Network, and perform \textit{forward sampling}. Such a method is used in autoregressive language models. Assume
\[P(\mathbf x) = \prod_{j=1}^n P(x_j | \text{Pa}(x_j))\]
Now, $y_j \sim P(y_j | y_1, \cdots, y_{j-1}, \mathbf x)  = P(y_j | s_j, \mathbf x)$. The $s_j$ is called the state in RNNs, and the calculation of probability is called softmax. \\
\begin{algorithm}[H]\label{alg:s-fs}
	\DontPrintSemicolon
	$x_1, \cdots, x_n \longleftarrow$ topologically sorted according to BN \;
	\For{$i = 1$ to $M$}{
		$\xi^i = [0, \cdots, 0]$\;
		\For{$j=1$ to $n$}{
			$\xi^i_j \sim P(x_j | \xi^i_{\text{Pa}(x_j)})$\;
		}
	}
	\Return{$\xi^1, \cdots, \xi^M$}
	\caption{Forward Sampling Algorithm}
\end{algorithm}
\begin{marginfigure}
	\centering
	\begin{tikzpicture}[main/.style = {draw, circle}] 
		\node[main] (a) {$x_1$}; 
		\node[main] (b) [right of=a] {$x_2$};
		\node[main] (c) [below = of $(a)!0.5!(b)$] {$x_3$};
		\node[main] (d) [below right of=c] {$x_4$};
		\node[main] (e) [below left of=d] {$x_5$};
		\draw[->] (a) -- (c);
		\draw[->] (b) -- (c);
		\draw[->] (c) -- (d);
		\draw[->] (d) -- (e);
		\draw[->] (c) -- (e);
	\end{tikzpicture}
	\caption{BN Example}
	\label{fig:s-exmp-1}
\end{marginfigure}
\begin{exmp}
Consider the BN shown in Figure \ref{fig:s-exmp-1}. We start with $x_1$ as it has no parents, and we want to get $\xi^1$. 
\begin{enumerate}
	\item $x_1 \sim P(x_1)$. Say $x_i$ were binary, and we get $\xi_1^1 = 0$.
	\item $x_2 \sim P(x_2)$. Say we get $\xi^1_2 = 1$. 
	\item $x_3 \sim P(x_3 | x_1 = 0, x_2 = 1)$. Say we get $\xi^1_3 = 1$.
\end{enumerate}
We can proceed similarly and get our sample $\xi^1$.
\end{exmp}
There are drawbacks of forward sampling - mainly being it will not be consistent when we have conditions imposed. Say we want to get the probability of $x_1$ being `what' when ! is the last token. Forward sampling would have most of the sampled sentences wasted since they don't end with `!'. Another example could be when we want to complete a missing attribute in a VAE network for object generation and in this case forward sampling wouldn't match the given values most of the time. \\
In such cases we use \textbf{importance sampling}. It is useful when it is hard to sample from $P(\mathbf x)$ or to lower the error in computation of the expected value of the function, i.e $\mathbb{E}_{P(\mathbf x)} [f(\mathbf x)]$, where $f(\mathbf x)$ has zeros at a large number of $\mathbf x$. Importance sampling samples from the important regions. \\
In importance sampling, we get to choose $Q(\mathbf x)$, called the proposal distribution, from which it is easy to generate samples. Designing such a $Q(\mathbf x)$ is problem-dependent. For example, in a language modeling task, $Q(\mathbf x)$ is the reverse language model. \\
Say we generate $S_Q = \{\mathbf x^1, \cdots, \mathbf x^M\}$ from $Q(\mathbf x)$. In general $\forall$ functions, $\mathbb{E}_{Q(\mathbf x)}[f(\mathbf x)] \neq \mathbb{E}_{P(\mathbf x)}[f(\mathbf x)] $. We use the following trick
\begin{align*}
	\mathbb{E}_{P(\mathbf x)}[f(\mathbf x)] &= \sum_{\mathbf x} f(\mathbf x)P(\mathbf x) \\
	&= \sum_{\mathbf x} Q(\mathbf x) \dfrac{P(\mathbf x)}{Q(\mathbf x)}f(\mathbf x) \qquad\text{let } W(\mathbf x) = \dfrac{P(\mathbf x)}{Q(\mathbf x)}\\
\mu_P(S_Q)	&= \dfrac{1}{M} \sum_{i=1}^M \big[ f(\mathbf x^i) W(\mathbf x^i) \big] \qquad\text{(Importance weighted estimate)}
\end{align*}
\begin{algorithm}[H]\label{alg:s-is}
	\DontPrintSemicolon
	\textbf{Given:} $M, Q(\mathbf x), P(\mathbf x)$\;
	\For{$i = 1$ to $M$}{
		$\xi^i \longleftarrow$ Sample from $Q(\mathbf x)$\;
		$W^i \longleftarrow \dfrac{P(\xi^i)}{Q(\xi^i)}$
	}
	\Return{$(\xi^1, W^1), \cdots, (\xi^M, W^M)$}
	\caption{Importance Sampling Algorithm}
\end{algorithm}
The user can decide what to do with the samples, for example
\begin{equation}
	\mathbb{E}_{P(\mathbf x)}[f(\mathbf x)] = \dfrac{1}{M} \sum_{i=1}^M f(\xi^i) W^i
\end{equation}
The limitations for the above algorithm is that it is not applicable for $P(\mathbf x)$ where we have an intractable normalizer (such as a CRF with a large tree width graph). In such cases, we do normalized importance sampling. We assume that
\begin{equation}
	P(\mathbf x) = \dfrac{\widetilde{P}(\mathbf x)}{Z} \quad \text{where }Z \text{ is intractable} 
\end{equation}
Then, we do
\begin{align*}
		\mathbb{E}_{P(\mathbf x)}[f(\mathbf x)] &= \dfrac{1}{Z} \sum_\mathbf{x} Q(\mathbf x) \dfrac{\widetilde{P}(\mathbf x)}{Q(\mathbf x)}f(\mathbf x) \\
		&= \dfrac{1}{Z} \sum_\mathbf{x} Q(\mathbf x) \widetilde{W}(\mathbf x)f(\mathbf x) \\
		Z &= \sum_{\mathbf x} \widetilde{P}(\mathbf x) 
\end{align*}
The applications for the above are
\begin{itemize}
	\item Undirected Graphical Model
	\item Given a BN, we want to sample of a subset of variables conditioned on fixed value of others, i.e $P(x_1, \cdots, x_r | x_{r+1}, \cdots, x_n = evidence)$
\end{itemize}
Given $S_Q = \{\mathbf x^1, \cdots, \mathbf x^M\}$, we have
\begin{equation}
	\mathbb{E}_P [f(\mathbf x)] \approx \dfrac{1}{ZM} \sum_{i=1}^M f(\mathbf x^i) \widetilde{W}(\mathbf x^i)
\end{equation}
where
\begin{eqnarray}
	Z = \sum_{\mathbf x \in \mathcal X} \widetilde{P}(\mathbf x) = \mathbb{E}_Q [\widetilde{W}(\mathbf x)] \approx \dfrac{1}{M} \sum_{i=1}^M \widetilde{W}(\mathbf x^i)
\end{eqnarray}
Thus,
\begin{equation}
	\mathbb{E}_P [f(\mathbf x)] \approx \dfrac{\sum_{i=1}^M f(\mathbf x^i) \widetilde{W}(\mathbf x^i)}{\sum_{i=1}^M \widetilde{W}(\mathbf x^i)}
\end{equation}
The choice of $Q(\mathbf x)$ for which the expected square error of the estimate from the true expected value is minimum when
\begin{equation}
	Q(\mathbf x) \propto |f(\mathbf x)| P(\mathbf x) , \quad Q(\mathbf x) > 0 \text{ whenever } P(\mathbf x) > 0
\end{equation}
Normalized importance sampling is biased when $M$ is small, and designing a good $Q(x)$ for which the sampling is efficient is not always easy.
\subsection{Markov Chain Monte Carlo Sampling}
In forward and importance sampling, each $\mathbf x^i$ was independent, and hence we could just run all instances in parallel. But MCMC sampling is a more general case, where we have dependencies in variables. It is useful when we cannot easily design the proposal distribution. MCMC sampling is more broadly applicable as we don't need very accurate proposal distribution or we don't know need to assume BN type factorization. \\
Such sampling is applicable when either of the following holds:
\begin{enumerate}
	\item It is easy to calculate conditional probability of 1 variable, i.e $P(x_i | \mathbf x_{-i})$ when the rest of the variables have fixed values.
	\item It is easy to calculate $\dfrac{P(\mathbf x)}{P(\mathbf x')}$ and normalizer is not required. Maybe a neural network is an example here.
\end{enumerate}
MCMC sampling is useful when everything else fails and guaranteed to converge to optimal when we take infinite samples. \\
Given $P(\mathbf x = x_1, \cdots, x_n)$ where $x_i \in \{1, \cdots, m\}$ is intractable to sample from but easy to evaluate.
\begin{enumerate}
	\item Gibbs Sampling - $P(x_i | \mathbf x - x_i) = P(x_i | \mathbf x_{-i})$
	\item Metropolis Hastings Sampling - $\dfrac{P(\mathbf x)}{P(\mathbf x')}$
\end{enumerate}
We need to design the \textit{MCMC Sampling Transition Function}. It is designed much like the proposal distribution. Thus,
\begin{equation}
\sum_{x \in X} T(x | x') = 1 \quad \text{where } T(x | x') \geq 0 \; \forall \; x, x' \in X \text{ and } |X| = m^n
\end{equation}
where $X$ is the space of all $x$. \\
\begin{algorithm}[H]\label{alg:s-mcmc}
	\DontPrintSemicolon
	Start with an initial sample $x^0$\;
	\For{$t = 1$ to $L$ where $L$ is large}{
		$x^{t+1} \sim T(x | x' = x^t)$ \;
		$x^0 \to x^1 \to x^2 \to \cdots \to x^L$ \;
	}
	Actually perform the sampling for $t = L+1$ to $t = L + Mk$\;
	$x^t \sim T(x | x^{t-1})$ \;
	\Return{$x^{L+k}, \cdots, x^{L+Mk}$}
	\caption{MCMC Sampling Algorithm}
\end{algorithm}
In Gibbs Sampling, $T(x|x')$ is defined as \\
$T(x|x') = 0$ if $x$ and $x'$ differ in more than one co-ordinate \\
$T(x|x') = \frac{1}{n} \sum_{i=1}^n P(x_i |  x'_{-i})$ if $x = x'$ \\
$T(x|x') = P(x_i | x_{-i}')$ if $x \neq x'$ \\
\begin{exmp}
	\begin{marginfigure}
		\centering
		\begin{tikzpicture}[->,main/.style = {draw, circle}] 
			\node[main] (a) {$2, 2$}; 
			\node[main] (b) [right of=a] {$1, 2$};
			\node[main] (c) [right of=b] {$2, 1$};
			\node[main] (d) [right of=c] {$1, 1$};
			\path
			(b) edge[bend left, above] (a)
			    edge[loop below] (b)
				edge[bend left, above] (d);
		\end{tikzpicture}
		\caption{Markov Chain}
		\label{fig:s-exmp-2}
	\end{marginfigure}
For $n = 2$ and $m = 2$. Thus each $x_i$ can either be 1 or 2. Thus, \\
$T(x|x' = [1, 2]) = 0$ if $x = [2, 1]$ \\
$T(x|x' = [1,2]) = \frac{1}{2} P(x_1 = 1 | x_2 = 2) + \frac{1}{2} P(x_2 = 2 | x_1 = 1)$ if $x = [1,2]$ \\
$T(x|x' = [1,2]) = P(x_1 = 2 | x_2 = 2)$ if $x = [2, 2]$ \\
$T(x|x' = [1,2]) = P(x_2 = 2 | x_1 = 1)$ if $x = [1, 2]$  \\
For the above transition probabilities, we can have a graph as shown in Figure \ref{fig:s-exmp-2}. 
\end{exmp}
Consider finite state space $X$, where we have states $1$ to $m^n$ and say we choose an arbitrary initial state $x^0$ at $t = 0$. We want to calcualte $T(x | x^0)$. This $T(x|x^0)$ is a multinomial distribution over all states. We sample from this. At $t = 0$, $P_{t=0}(x) = 1$ if $x = x^0$ and $0$ else. At $t = 1$, we want $P_{t=1}(x) = T(x | x^0)$. At $t=2$, the probability of being in state $x$ is $P_{t=2}(x) = \sum_{x'} T(x | x') T(x' | x^0) = \sum_{x'} T(x|x') P_{t-1}(x')$. Thus in general
\begin{equation}
	P_t(x) = \sum_{x'} T(x|x') P_{t-1}(x')
\end{equation}
As $t \to \infty$, $P_{t+1}(x) \approx P_t(x)$ for convergence as $t\to\infty$ and this is equal to the stationary distribution $\pi(x)$. We are interested in $T(x|x')$ for which $P_{t+1}(x) = \sum_{x'} P_t(x') T(x|x')$ and as $t \to \infty$
\begin{equation}
	\pi(x) = \sum_{x'} \pi(x') T(x|x')
\end{equation}
has a unique solution for given $T(x|x')$ and $\pi(x)$ should be reachable from any initial state $x^0$ via Markov walks using $T(x|x')$. \\
\begin{rem}
$P_t(x)$ is the probability of being in state $x$ after $T$ MCMC steps, but it is conditioned on $x_0$, i.e the initial state.
\end{rem}
\begin{rem}
	It is not necessary that $\pi(x)$ is unique, but it can be proven that for the transition matrix $T(\cdot)$, we will at least get 1 stationary distribution.
\end{rem}
\begin{exmp}[Single Solution]
Consider a state space such that $|\mathcal X| = 3$ and we have
\[T = \begin{bmatrix}
	0.25 & 0.5 & 0 \\
	0.75 & 0 & 0.75 \\
	0 & 0.5 & 0.25
\end{bmatrix}\]
In the matrix, the columns denote $x'$ and rows denote $x$, thus we should sum to $1$ over all columns. \\
\[\pi(x) = [\pi_1, \pi_2, \pi_3]\]
\begin{align*}
	\pi(x) &= \sum_{x'} \pi(x') T(x|x') \\
	\pi_1 &= 0.25\pi_1 + 0.5\pi_2 \\
	\pi_2 &= 0.75\pi_1 + 0.75\pi_3 \\
	\pi_3 &= 0.5\pi_2 + 0.25\pi_3 \\
	\pi_1 &+ \pi_2 + \pi_3 = 1
\end{align*}
We get
\[\pi_1 = \frac{2}{7}, \pi_2 = \frac{3}{7}, \pi_3 = \frac{2}{7}\]
Notice that $1$ and $3$ are symmetric, as seen in $T$ too.
\end{exmp}
\begin{exmp}[Multiple Solutions]
Say that
\[
T(x|x') = \begin{cases}
	1 \text{ if } x = x' \\
	0 \text{ otherwise}
\end{cases}
\]
Any stationary distribution is valid, and $T$ is an identity matrix. Thus, infinite number of solutions and 
\[P_t = \delta(x^0)\]
\end{exmp}
\begin{exmp}[Unreachable Solution]
Even when $T(x|x')$ has a unique stationary distribution $\pi_T(x)$, this distribution may not be the one we can reach or converge to via MCMC walks.
\[
T(x|x' ) = \begin{cases}
	1 \text{ if } x \neq x' \\
	0 \text{ otherwise}
\end{cases}
\]
$\pi_1 = \pi_2, \pi_2 = \pi_1$ and $\pi_ 1 + \pi_2 = 1$. Thus we get
\[\pi_1 = \pi_2 = \frac{1}{2}\]
This is an unreachable solution since we aren't guaranteed to reach it via MCMC sampling. Say
\begin{align*}
	&t = 0 \quad x^0 = v_1 \quad t \text{ is even } x^t = v_1 \\
	&t = 1 \quad x^1 = v_2 \quad t\text{ is odd } x^t = v_2
\end{align*}
\end{exmp}
\begin{defn}[Ergodicity]
Markov chains with a unique $\pi(x)$ that can be reached via MCMC steps irrespective of starting point are called ergodic.
\end{defn}
\begin{defn}[Regular]
A Markov chain is regular if there exists a $k$ such that $x$ to $x'$ can be reached in exactly $k$ steps.
\end{defn}
\begin{thm}
When number of states is finite, then a Markov chain is ergodic iff it is regular.
\end{thm}
\begin{proof}
Skipped.
\end{proof}
\begin{exmp}
In the single solution example above, let us see what $k$ means. For $k=1$, $v_1 \to v_3$ not possible in $1$ step and thus, it is not ergodic for $k=1$. Now for $k = 2$ we need to check all possibile transitions and we find that the Markov chain is regular with $k=2$.
\end{exmp}
Manually checking might get cumbersome, and we have a simpler sufficient condition as follows - 
\begin{lem}
A finite state Markov chain is regular when 
\begin{enumerate}
	\item every state has a self loop with non-zero probability
	\item between any two states $\exists$ a path of non-zero probability
\end{enumerate}
\end{lem}
Note that the condition above is $when$ and not $iff$. 

\begin{thm}
Markov chains defined by Gibbs sampling is ergodic, and has a stationary distribution on $P(\mathbf x)$ when $P(\mathbf x)$ is positive.
\end{thm}
\begin{proof}
Recall that
\[
T(x|x') = \begin{cases}
	0 \text{ if } x \text{ and } x' \text{ differ in more than one coordinate} \\
	\frac{1}{n} \sum_{i=1}^n P(x_i | x'_{-i}) \text{ if } x = x' \\
	P(x_i | x_{-i}') \text{ if } x \neq x', \text{ i.e } x_{-i} = x'_{-i}
\end{cases}
\]
Since $P(\mathbf x)$ is positive, self-loop probability is positive as $\forall \; \mathbf x$ $T(\mathbf x | \mathbf x')> 0$.
We can reach from $\mathbf x' \to \mathbf x$ with non-zero probability in $n$ steps at maximum. Now to show $\pi(\mathbf x) = P(\mathbf x)$.
\begin{align*}
	\pi(\mathbf x) &= \sum_{\mathbf x'} T(\mathbf x | \mathbf x') \\
	&= \sum_{\mathbf x'} P(\mathbf x') T(\mathbf x | \mathbf x') \\
	&= \sum_{x_1'} P(x_1', x_2, \cdots, x_n) \frac{1}{n} P(x_1, \cdots, x_n) + \cdots \\&\quad+ \sum_{x_n'} P(x_1, \cdots, x_{n-1}, x_n') \frac{1}{n} P(x_n | x_1, \cdots, x_{n-1}) \\
	&= P(x_2, \cdots, x_n) \frac{1}{n} P(x_1, \cdots x_n) + \cdots \\&\quad+ \frac{1}{n} P(x_1, \cdots x_{n-1}) P(x_n | x_1, \cdots x_{n-1}) \\
	&= \frac{1}{n} P(x_1, \cdots, x_n) + \cdots + \frac{1}{n}P(x_1, \cdots, x_n) \\
	&= P(x_1, \cdots, x_n)
\end{align*} 
\end{proof}
In undirected graphical models, it is easy to find the probabilities.
\[
P(x_1, \cdots, x_n) = \dfrac{1}{Z} \prod_C \psi_C (\mathbf x_C)
\]
\begin{equation}
	\begin{split}
		P(x_i | \mathbf x_{-i}) &= \dfrac{P(x_1, \cdots, x_n)}{\sum_{x_i} P(x_1, \cdots, x_n)} \\ &= \dfrac{\prod_C \psi_C (\mathbf x_C)}{\sum_{x_i} \prod_C \psi_C(\mathbf x_C)} = \dfrac{\prod_{C: i \in \mathcal C} \psi_C(\mathbf x_C)}{\sum_{x_i}\prod_{C: i \in \mathcal C} \psi_C(\mathbf x_C)}
	\end{split}
	\end{equation}
\begin{marginfigure}
	\centering
	\begin{tikzpicture}[main/.style = {draw, circle}] 
		\node[main] (1) {$1$}; 
		\node[main] (2) [above right of=1] {$2$}; 
		\node[main] (3) [below right of=1] {$3$}; 
		\node[main] (4) [below right of=2] {$4$}; 
		\draw[-] (1) -- (2);
		\draw[-] (4) -- (3);
		\draw[-] (1) -- (3);
		\draw[-] (2) -- (4);
		\draw[-] (3) -- (2);
	\end{tikzpicture}
	\caption{Sample UGM}
	\label{fig:s-exmp-3}		
\end{marginfigure}
\begin{exmp}
Say we have the undirected graphical model as shown in Figure \ref{fig:s-exmp-3}. In this case, we can write
\[
\begin{split}
	P(x_1 | \mathbf x_{-1}) &= \dfrac{\psi_{123}(x_1, x_2, x_3)\psi_{234}(x_2, x_3, x_4)}{\sum_{x_1}\psi_{123}(x_1, x_2, x_3)\psi_{234}(x_2, x_3, x_4)} \\
	&= \dfrac{\psi_{123}}{\sum_{x_1} \psi_{123}(x_1, x_2, x_3)}
\end{split}
\]
\end{exmp}
\textbf{Limitations of Gibbs}
\begin{enumerate}
	\item For a continuous distribution, $P(x_i | x_{-i})$ may not be easy to obtain
	\item When variables are highly correlated, Gibbs sampling which only had local moves might have high probability states whose neighbors are mostly low probability leading to poor mixing.
\end{enumerate}
\begin{exmp}[Poor mixing]
Say we have
\[
P(x_1, x_2) = \begin{cases}
	\dfrac{1- \epsilon}{2} \text{ if } x_1 = x_2, \; x_i \in \{0,1\}\\
	\dfrac{\epsilon}{2} \text{ if } x_1 \neq x_2 \; \text{ and } \epsilon \text{ is small}
\end{cases}
\]
Say we compute $T([0,0] | [0, 0])$.
\[T([0, 0] | [0, 0]) = \dfrac{1}{2} \Big[P(x_1 = 0| x_2 = 0) + P(x_2 = 0 | x_1 = 0) \Big]= 1- \epsilon\]
By symmetry $T([1, 0] | [0, 0]) = T([0, 1] | [1, 1]) = \frac{\epsilon}{2}$ and when $\epsilon \to 0$, we mostly will get [0, 0] and [1, 1] states. To go to [1, 1] from [0, 0] we need to take a low probability path, and thus we have high probability islands with no direct transition between them. 
\end{exmp}
\subsection{Metropolis Hastings Sampling}
This is motivated by the need to design moves that go from one high probability state to another without passing through low probability states. In this algorithm, $T(\cdot)$ isn't the sole determiner of transition, but is a proposal distribution for transitions. The Metropolis Hastings Algorithm has that
\begin{enumerate}
	\item Choose any proposal distribution for transferring from $x$ to $x'$ i.e $T^Q(x \to x')$
	\item Use $T^Q$ to propose a transition from $x \to x'$. We accept the proposal with probability $A(x \to x')$ and transition or stay in $x$. Then we define
	\begin{align}
		T(x\to x') &= T^Q (x \to x') A(x \to x') \quad x \neq x' \\
		T(x \to x') &= T^Q (x \to x) + \sum_{x' \neq x} T^Q (x\to x') (1 - A(x \to x'))
	\end{align}
\end{enumerate}
\begin{defn}[Reversible chain]
A finite state Markov Chain $T$ is reversible, if $\exists$ a unique $\pi$ such that $\forall \; x, x' \in \mathcal X$
\begin{equation}
	\pi(x') T(x' \to x) = \pi (x) T(x \to x')
\end{equation}
The above equation is called the \textbf{Detailed Balance Equation (DBE)}.
\end{defn}
\begin{thm}
If $\pi(x)$ satisfies the above equation, then $\pi(x)$ is a stationary distribution of $T$.
\end{thm}
\begin{proof}
$\sum_{x'} \pi(x') T(x' \to x) = \pi(x) \sum_{x'} T(x \to x') = \pi (x)$
\end{proof}
\begin{quest}
Show that the transition function for Gibbs Sampling satisfies \textbf{DBE}, i.e
\[P(\mathbf x) T(\mathbf x' | \mathbf x) = P(\mathbf x') T(\mathbf x | \mathbf x')\]
\end{quest}
\begin{ans}
Note that since we are working with the Gibbs transition function, $\mathbf x$ and $\mathbf x'$ can differ only in one position. Let that position be $i$. Then for $\mathbf x' = x_i'$ we have
\begin{align*}
	P(\mathbf x) T(\mathbf x' | \mathbf x) &= \dfrac{1}{n} P(\mathbf x) T(x_i' | \mathbf x) \\ 
	&= \dfrac{1}{n} P(\mathbf x) \dfrac{P(x_i', \mathbf x_{-i})}{\sum_{x_i'}P(x_i', \mathbf x_{-i})} \\
	&= \dfrac{1}{n} P(\mathbf x)  \dfrac{P(\mathbf x')}{\sum_{x_i'}P(x_i', \mathbf x_{-i})} \\
	&= P(\mathbf x') T(\mathbf x | \mathbf x')
\end{align*}
Hence, notice that $P(\mathbf x)$ is the stationary distribution under Gibbs Sampling.
\end{ans}
Now, a task is to choose $A$. We need to design $A$ to satisfy \textbf{DBE} for $x \neq x'$.
\[\pi(x)T^Q(x\to x') A(x\to x') = \pi(x') T^Q (x' \to x) A(x' \to x)\]
\begin{equation}
	A(x \to x') = \min\bigg[1, \dfrac{\pi(x')T^Q(x' \to x)}{\pi(x) T^Q(x\to x')}\bigg]
\end{equation}
Given a desired stationary distribution $P(\mathbf x)$, designing the $A(\cdot)$ just requires the user provided $T^Q$ and the ratio of probabilities $\frac{P(\mathbf x')}{P(\mathbf x)}$. The proof that the above equation works is not tough to see. It is easy to see that either
$\pi(x')T^Q(x' \to x) \geq \pi(x) T^Q(x\to x')$ or $\pi(x')T^Q(x' \to x) < \pi(x) T^Q(x\to x')$. In either case if $A(x \to x') < 1$, then $A(x' \to x) = 1$. Hence, just substituting with this result in \textbf{DBE} gives the required result.

\begin{exmp}
Say we want a stationary distribution
\[\pi = [\pi_1, \pi_2, \pi_3] = \bigg[\dfrac{2}{Z}, \dfrac{3}{Z}, \dfrac{2}{Z} \bigg]\]
In this, it is trivial to see $Z = 7$, but in more complex cases, it might not be. So we leave it as $Z$. Say we choose an arbitrary $T^Q$ and compute $A$. Say we have a uniform one, i.e
\[T^Q (x \to x') = \dfrac{1}{3}\]
We can see that \[A(1 \to 2) = \min\bigg[1, \dfrac{\pi(2) T^Q (1|2)}{\pi(1) T^Q (2|1)} \bigg] = \min\bigg[1, \dfrac{3}{2}\bigg] = 1\]
Similarly, $A(2 \to 3) = 2/3$.
\end{exmp}
\subsection{Langevin Monte-Carlo}
Sampling from an arbitrary differential function, say a neural network representing $P(\mathbf x)$. For example in audio, images. We write
\begin{equation}
	P(\mathbf x) = \dfrac{e^{-E_\theta(\mathbf x)}}{Z}
\end{equation}
where $E_\theta(\mathbf x) \mapsto \mathbb R$ is an arbitrary differentiable function in $\mathbf x$ and $Z_\theta$ is intractable to compute. Given two $\mathbf x$ and $\mathbf x'$, it is easy to compute the ratio
\begin{equation}
	\dfrac{P(\mathbf x)}{P(\mathbf x')} = \dfrac{e^{-E_\theta(\mathbf x)}}{e^{-E_\theta(\mathbf x')}}
\end{equation}
To design $T^Q(\mathbf x' | \mathbf x)$, we use the intuition that transitioning from any $\mathbf x$ to $\mathbf x'$ along directions of maximum increase in $\log P(\mathbf x)$ by using gradients.
\begin{equation}
	\mathbf x' = \mathbf x + \tau \nabla_\mathbf{x} \log P(\mathbf x) = \mathbf x - \tau \nabla_\mathbf x E_\theta(\mathbf x)
\end{equation}
To make the transitions probabilistic, we add small Gaussian noise.
\begin{equation}
	\mathbf x' = \mathbf x - \tau \nabla_\mathbf x E_\theta(\mathbf x) + \sqrt{2\tau} \xi
\end{equation}
where $\xi \sim \mathcal{N}(\mathbf 0, I_d)$. With this, we can write
\begin{align}
	T^Q(\mathbf x \to \mathbf x') &= \dfrac{1}{\sqrt{2\pi} 2\tau} \exp\bigg(-\dfrac{1}{4\tau} \|\mathbf x' - \mathbf x + \tau\nabla_\mathbf{x} E_\theta(\mathbf x) \|^2\bigg) \\
	A(\mathbf x \to \mathbf x') &= \min\bigg\{1, \dfrac{e^{-E_\theta(\mathbf x')} T^Q(\mathbf x' \to \mathbf x)}{e^{-E_\theta(\mathbf x)}T^Q(\mathbf x \to \mathbf x')}\bigg\}
\end{align}